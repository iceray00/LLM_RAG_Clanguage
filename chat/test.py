from haystack.components.builders import ChatPromptBuilder
from haystack_integrations.components.generators.ollama import OllamaChatGenerator
from haystack.dataclasses import ChatMessage, Document
from haystack import Pipeline

from haystack_integrations.document_stores.chroma import ChromaDocumentStore
from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever
import json
from pathlib import Path
from tqdm import tqdm

# è·å–JSONæ–‡ä»¶çš„è·¯å¾„
json_file_path = Path("data") / "Simulated_Test.json"  # å‡è®¾JSONæ–‡ä»¶åæ˜¯ Simulated_Test.json

# è¯»å–å¹¶è§£æJSONæ–‡ä»¶
try:
    with open(json_file_path, "r", encoding="utf-8") as f:
        knowledge_data = json.load(f)
except FileNotFoundError:
    print("Error: JSON file not found.")
    exit(1)

# åˆ›å»º ChromaDocumentStore å’Œ æ£€ç´¢å™¨
document_store = ChromaDocumentStore()
retriever = ChromaQueryTextRetriever(document_store=document_store)

# ç”Ÿæˆ Haystack æ–‡æ¡£
documents = []
for item in knowledge_data:
    doc = Document(
        content=item["title"],
        meta={
            "é—®é¢˜/å†…å®¹": item["title"],
            "ç­”æ¡ˆ/å†…å®¹": item["content"],
            "éƒ¨åˆ†": item["part"]["section"],
            "é¢˜å‹/ç±»å‹": item["part"]["classification"],
            "å…³é”®è¯": ", ".join(item["tags"]) if isinstance(item["tags"], list) else item["tags"]
        }
    )
    documents.append(doc)

# å°†æ–‡æ¡£å†™å…¥ ChromaDocumentStore
for doc in tqdm(documents, desc="Indexing documents", unit="document"):
    document_store.write_documents([doc])

# åˆ›å»º ChatPromptBuilder å’Œ OllamaChatGenerator
prompt_builder = ChatPromptBuilder()
generator = OllamaChatGenerator(
    model="qwen2:latest",
    url="http://localhost:11434",
    generation_kwargs={
        "temperature": 0.9,

    }
)

# è®¾ç½®å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡
conversation_history = [
    ChatMessage.from_system("ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘ä¼šæé—®ä¸€äº›å…³äºç¼–ç¨‹å’ŒæŠ€æœ¯æ–¹é¢çš„é—®é¢˜ï¼Œä½ è¦æä¾›æœ‰å¸®åŠ©çš„ç­”æ¡ˆã€‚")
]


# åˆ›å»º Pipeline
pipe = Pipeline()
pipe.add_component("prompt_builder", prompt_builder)
pipe.add_component("llm", generator)
pipe.connect("prompt_builder.prompt", "llm.messages")


def chat_with_RAG(question, conversation_history, retriever, top_k=4):
    # é¦–å…ˆï¼Œä½¿ç”¨ retriever è·å–ç›¸å…³æ–‡æ¡£
    retrieved_documents = retriever.run(query=question, top_k=top_k)["documents"]

    # print(f"\n## Retriever Answer: (top_k = {top_k})##\n")
    # for d in retrieved_documents:
    #     print(d.meta, d.score)

    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ·»åŠ åˆ°å†å²å¯¹è¯ä¸­
    conversation_history.append(ChatMessage.from_system("ç›¸å…³æ–‡æ¡£å¦‚ä¸‹ï¼š"))
    for doc in retrieved_documents:
        conversation_history.append(ChatMessage.from_system(f"{doc.meta['é—®é¢˜/å†…å®¹']}: {doc.content}"))

    # è·å–ç”Ÿæˆå™¨çš„å›ç­”
    querying = pipe.run(data={
        "prompt_builder": {
            "template_variables": {"question": question},
            "template": conversation_history + [ChatMessage.from_user(question)]
        }
    })

    # è·å–å›ç­”å¹¶æ›´æ–°å†å²å¯¹è¯
    response = querying["llm"]["replies"]
    response_text = response[0].text
    conversation_history.append(ChatMessage.from_assistant(response_text))

    return response_text, conversation_history


# äº¤äº’å¼å¯¹è¯å¾ªç¯
print("\nğŸŸ¢ å¯¹è¯RAGå·²å°±ç»ªï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
i = 1
while True:
    try:
        print(f"\n\n## Round {i} ##\n")
        i += 1
        question = input("\nâ“ ä½ çš„é—®é¢˜: ")
        if question.lower() in ["exit", "quit"]:
            print("é€€å‡ºå¯¹è¯ã€‚")
            break

        # å¤„ç†é—®é¢˜å¹¶è°ƒç”¨ retriever è¿›è¡Œæ–‡æ¡£æ£€ç´¢
        answer, conversation_history = chat_with_RAG(
            question=question,
            conversation_history=conversation_history,
            retriever=retriever
        )

        # è¾“å‡ºå›ç­”
        print(f"\nğŸ¤– åŠ©æ‰‹å›ç­”:\n{answer}")

    except KeyboardInterrupt:
        print("\nå¯¹è¯ä¸­æ–­ã€‚")
        break
