import argparse
import time
import json
import os

from haystack.components.builders import ChatPromptBuilder
from haystack_integrations.components.generators.ollama import OllamaChatGenerator
from haystack.dataclasses import ChatMessage, Document
from haystack import Pipeline
from haystack_integrations.document_stores.chroma import ChromaDocumentStore
from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever
from pathlib import Path
from tqdm import tqdm

# è·å–å½“å‰æ—¶é—´æˆ³
timestamp = time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime())
output_folder = Path("chat_records") / timestamp
output_folder.mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ–èŠå¤©è®°å½•åˆ—è¡¨
chat_records = []


def main():
    parser = argparse.ArgumentParser(description="RAG Chat Bot for C Language Learning!")

    parser.add_argument("-e", "--Emodel", type=str, default="nomic-embed-text:latest", help="Embedding Model")
    parser.add_argument("-g", "--Gmodel", type=str, default="qwen2:latest", help="Generator Model")

    # parser.add_argument("--input_file", type=str, help="input one file path for Contract source code")
    # parser.add_argument("--o", "-output", type=str, default="./vfcs/", help="output path for VFCS")
    # parser.add_argument("--model_name", type=str, default="Qwen1.5-32B-Q4", help="model name")
    # parser.add_argument("--ctx", type=int, default=default_ctx, help="The maximum length of the context")
    # parser.add_argument("--prompt5", type=str, default=None, help="Prompt5")
    # parser.add_argument("--prompt6", type=str, default=None, help="Prompt6")

    args = parser.parse_args()
    #
    # if args.mode == "knowledge":
    #     try:
    #         with open(Path("data") / "Summary_of_Knowledge_Points.json", "r", encoding="utf-8") as f:
    #             knowledge_data = json.load(f)
    #     except FileNotFoundError:
    #         print("Error: JSON file not found.")
    #         exit(1)
    # elif args.mode == "test":
    #     try:
    #         with open(Path("data") / "Simulated_Test.json", "r", encoding="utf-8") as f:
    #             knowledge_data = json.load(f)
    #     except FileNotFoundError:
    #         print("Error: JSON file not found.")
    #         exit(1)
    # else:
    #     print("Error: Invalid mode. '-m' '--mode' Should input `test` or `knowledge`.")
    #     exit(1)

    # Embedding Model
    if args.Emodel == "default":
        document_store = ChromaDocumentStore()
        print("Now, Embedding Model is: all-MiniLM-L6-v2")
    else:
        # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥å‡½æ•°åˆ›å»º ChromaDocumentStore
        document_store = ChromaDocumentStore(
            embedding_function="OllamaEmbeddingFunction",
            url="http://localhost:11434/api/embeddings",
            model_name=args.Emodel
        )
        print(f"### Embedding Model is: {args.Emodel} ###")

    print(f"### Generator Model is: {args.Gmodel} ###")

    # retriever = ChromaQueryTextRetriever(document_store=document_store)

    # documents = []
    # for item in knowledge_data:
    #     doc = Document(
    #         content=item["title"],
    #         meta={
    #             "é—®é¢˜/å†…å®¹": item["title"],
    #             "ç­”æ¡ˆ/å†…å®¹": item["content"],
    #             "éƒ¨åˆ†": item["part"]["section"],
    #             "é¢˜å‹/ç±»å‹": item["part"]["classification"],
    #             "å…³é”®è¯": ", ".join(item["tags"]) if isinstance(item["tags"], list) else item["tags"]
    #         }
    #     )
    #     documents.append(doc)

    # # å°†æ–‡æ¡£å†™å…¥ ChromaDocumentStoreçš„éƒ¨åˆ†ä¿æŒä¸å˜
    # for doc in tqdm(documents, desc="Indexing documents", unit="document"):
    #     document_store.write_documents([doc])

    # åˆ›å»º ChatPromptBuilder å’Œ OllamaChatGeneratorçš„éƒ¨åˆ†ä¿æŒä¸å˜
    prompt_builder = ChatPromptBuilder()
    generator = OllamaChatGenerator(
        model="qwen2:latest",
        # url="http://localhost:11434/api/chat",
        url="http://localhost:11434/",
        generation_kwargs={
            "temperature": 0.9,
        }
    )

    pipe = Pipeline()
    pipe.add_component("prompt_builder", prompt_builder)
    pipe.add_component("llm", generator)
    pipe.connect("prompt_builder.prompt", "llm.messages")


    # ç¬¬ä¸€è½®
    conversation_history = [
        ChatMessage.from_system("ä½ æ˜¯ä¸€ä¸ªç²¾é€šCè¯­è¨€çš„æ™ºèƒ½åŠ©æ•™ã€‚ç”¨æˆ·ä¼šæé—®ä¸€äº›å…³äºCè¯­è¨€ç¼–ç¨‹å’ŒæŠ€æœ¯æ–¹é¢çš„é—®é¢˜ï¼Œä½ è¦æä¾›æœ‰å¸®åŠ©çš„ç­”æ¡ˆã€‚åŒæ—¶ï¼Œå½“å‰åœºæ™¯æ˜¯â€œä»£ç è§£é‡Šâ€ï¼Œåªéœ€è¦ä¸“æ³¨äºå¯¹ç”¨æˆ·æå‡ºçš„ä»£ç ç‰‡æ®µè¿›è¡Œè¯¦å°½çš„è§£é‡Šå³å¯ï¼")
    ]

    def chat_with_RAG_first_time(question, conversation_history):

        template = (
"### **é—®é¢˜**\n" + question + """\n### **æ³¨æ„äº‹é¡¹**ï¼š  
   1. ç®€è¦è¯´æ˜ä»£ç çš„ä¸»è¦åŠŸèƒ½å’Œæ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬å…³é”®éƒ¨åˆ†å¦‚å‡½æ•°ã€å˜é‡ã€å¾ªç¯ã€æ¡ä»¶åˆ¤æ–­ç­‰ã€‚  
   2. æ¢³ç†å‡½æ•°çš„è°ƒç”¨å…³ç³»åŠå…¶ä½œç”¨ï¼Œåˆ†æå¾ªç¯å’Œæ¡ä»¶åˆ¤æ–­é€»è¾‘å¯¹ç¨‹åºæµç¨‹çš„å½±å“ã€‚  
   3. å¦‚æ¶‰åŠè¾“å…¥è¾“å‡ºæˆ–æ•°æ®å¤„ç†ï¼Œè¯·æè¿°å…¶æ–¹å¼åŠæ„ä¹‰ã€‚  
   4. è¯†åˆ«å¯èƒ½å­˜åœ¨çš„é‡å¤é€»è¾‘æˆ–åŠŸèƒ½ï¼Œè‹¥æœ‰ï¼Œæå‡ºä¼˜åŒ–å»ºè®®ã€‚  
   5. è¯·ç¡®ä¿æè¿°æ¸…æ™°æ˜äº†ï¼Œé¿å…å†—é•¿å’Œå¤æ‚è¡¨è¿°ã€‚  

**æ³¨æ„**ï¼š
å¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºä»£ç ï¼Œè€Œæ˜¯è¿›è¡Œé—®å·æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œä»£ç è§£é‡Šâ€åœºæ™¯ï¼Œéœ€è¦è¾“å…¥ä»£ç ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦è¿›è¡Œè™šç©ºä»£ç æé€ ï¼

""")

        # è·å–ç”Ÿæˆå™¨çš„å›ç­”
        querying = pipe.run(data={
            "prompt_builder": {
                "template_variables": {"question": question},
                "template": conversation_history + [ChatMessage.from_user(template)]
            }
        })

        # è·å–å›ç­”å¹¶æ›´æ–°å†å²å¯¹è¯
        response = querying["llm"]["replies"]
        response_text = response[0].text
        conversation_history.append(ChatMessage.from_assistant(response_text))

        return response_text, conversation_history

    def chat_with_RAG_second_time(question, conversation_history):
        template = (
"è¯·ä½ ç»“åˆæ­¤å‰ç»™å‡ºçš„è¯­è¨€ä»£ç æ ¼å¼ï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„é—®é¢˜éœ€æ±‚è¿›è¡Œè¯¦ç»†çš„è§£é‡Šå’Œåˆ†æã€‚è¯·ç»“åˆå‰é¢çš„ä»£ç å¯¹æ–°ä»£ç è¿›è¡Œåˆ†æï¼š\n" +
"### **é—®é¢˜**\n" + question + """\n
   è¯·åˆ¤æ–­æ–°ä»£ç ä¸å‰é¢ä»£ç çš„å…³ç³»å¹¶åˆ†æï¼š  
   1. **å»¶ç»­æˆ–è¡”æ¥**ï¼šè¯´æ˜æ–°å¢éƒ¨åˆ†ä¸å‰é¢ä»£ç çš„é€»è¾‘è¡”æ¥åŠæ‰©å±•åŠŸèƒ½ï¼Œåˆ†ææ–°å¢å˜é‡æˆ–å‡½æ•°çš„ä½œç”¨ã€‚  
   2. **æ–°åŠŸèƒ½æˆ–æ¨¡å—**ï¼šå¦‚ä¸ºæ–°å¢å†…å®¹ï¼Œè¯·ç‹¬ç«‹åˆ†æå…¶åŠŸèƒ½æ„ä¹‰åŠå…¶åœ¨æ•´ä½“ç¨‹åºä¸­çš„ä½œç”¨ã€‚  
   3. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¦‚æ¶‰åŠä¼˜åŒ–ï¼Œè¯„ä¼°ä¼˜åŒ–çš„å¿…è¦æ€§åŠå…¶å¯¹æ‰§è¡Œæ•ˆç‡æˆ–åŠŸèƒ½çš„æå‡ã€‚  
   4. **ä¿®æ”¹æˆ–é‡æ„**ï¼šå¦‚ä¸ºä¿®æ”¹æˆ–é‡æ„ä»£ç ï¼Œè¯·æ‰¾åˆ°é”™è¯¯å¹¶ä¿®æ”¹ï¼Œåˆ†ææ”¹åŠ¨å¯¹å¯è¯»æ€§åŠæ€§èƒ½çš„å½±å“ã€‚  
   5. **è°ƒè¯•ä¸ä¿®å¤**ï¼šå¦‚ä¸ºè°ƒè¯•ä»£ç ï¼Œéœ€è¯´æ˜å…¶å¯¹æ­£å¸¸ç¨‹åºçš„å½±å“ï¼Œç¡®è®¤æ˜¯å¦å­˜åœ¨è¿›ä¸€æ­¥æ¸…ç†æˆ–ä¼˜åŒ–ç©ºé—´ã€‚

**æ³¨æ„**ï¼š
å¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºä»£ç ï¼Œè€Œæ˜¯è¿›è¡Œé—®å·æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œä»£ç è§£é‡Šâ€åœºæ™¯ï¼Œéœ€è¦è¾“å…¥ä»£ç ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦è¿›è¡Œè™šç©ºä»£ç æé€ ï¼

""")
        # å®Œæ•´çš„prompt
        querying = pipe.run(data={
            "prompt_builder": {
                "template_variables": {"question": question},
                "template": conversation_history + [ChatMessage.from_user(template)]
            }
        })

        # è·å–å›ç­”å¹¶æ›´æ–°å†å²å¯¹è¯
        response = querying["llm"]["replies"]
        response_text = response[0].text
        conversation_history.append(ChatMessage.from_assistant(response_text))

        return response_text, conversation_history

    # äº¤äº’å¼å¯¹è¯å¾ªç¯
    print("\nğŸŸ¢ å¯¹è¯RAGå·²å°±ç»ªï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
    i = 1

    while True:
        try:
            print(f"\n## Round {i} ##")
            i += 1
            question = input("\nâ“ ä½ çš„é—®é¢˜: \n")
            if question.lower() in ["exit", "quit"]:
                print("é€€å‡ºå¯¹è¯ã€‚")
                break

            # ä¿å­˜å½“å‰çš„ç³»ç»Ÿä¿¡æ¯åˆ°èŠå¤©è®°å½•
            chat_record = {
                "timestamp": time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime()),
                "user_input": question,
                "system_response": ""
            }

            if i == 1:
                # å¼€å§‹ç¬¬ä¸€è½®å¯¹è¯
                answer, conversation_history = chat_with_RAG_first_time(
                    question=question,
                    conversation_history=conversation_history
                )

            else:
                # éç¬¬ä¸€è½®å¯¹è¯
                answer, conversation_history = chat_with_RAG_second_time(
                    question=question,
                    conversation_history=conversation_history,
                )

            # æ›´æ–°èŠå¤©è®°å½•ä¸­çš„ç³»ç»Ÿå“åº”
            chat_record["system_response"] = answer

            # å°†å½“å‰èŠå¤©è®°å½•æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            chat_records.append(chat_record)

            # å°†èŠå¤©è®°å½•ä¿å­˜åˆ° JSON æ–‡ä»¶
            with open(os.path.join(output_folder, "chat_history.json"), "w", encoding="utf-8") as f:
                json.dump(chat_records, f, ensure_ascii=False, indent=2)

            # è¾“å‡ºå›ç­”
            print(f"\nğŸ¤– åŠ©æ‰‹å›ç­”:\n{answer}")

        except KeyboardInterrupt:
            print("\nå¯¹è¯ä¸­æ–­ã€‚")
            break


if __name__ == "__main__":
    main()
