import argparse
import time
import json
import os

from haystack.components.builders import ChatPromptBuilder
from haystack_integrations.components.generators.ollama import OllamaChatGenerator
from haystack.dataclasses import ChatMessage, Document
from haystack import Pipeline
from haystack_integrations.document_stores.chroma import ChromaDocumentStore
from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever
from pathlib import Path
from tqdm import tqdm

# è·å–å½“å‰æ—¶é—´æˆ³
timestamp = time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime())
output_folder = Path("chat_records") / timestamp
output_folder.mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ–èŠå¤©è®°å½•åˆ—è¡¨
chat_records = []


def main():
    parser = argparse.ArgumentParser(description="RAG Chat Bot for C Language Learning!")

    parser.add_argument("-e", "--Emodel", type=str, default="nomic-embed-text:latest", help="Embedding Model")
    parser.add_argument("-g", "--Gmodel", type=str, default="qwen2:latest", help="Generator Model")
    parser.add_argument("-k", "--top_k", type=int, default=5, help="Top-k for retrieval")
    parser.add_argument("-m", "--mode", type=str, default="knowledge", help="`test` or `knowledge`. Default is `knowledge`")

    # parser.add_argument("--input_file", type=str, help="input one file path for Contract source code")
    # parser.add_argument("--o", "-output", type=str, default="./vfcs/", help="output path for VFCS")
    # parser.add_argument("--model_name", type=str, default="Qwen1.5-32B-Q4", help="model name")
    # parser.add_argument("--ctx", type=int, default=default_ctx, help="The maximum length of the context")
    # parser.add_argument("--prompt5", type=str, default=None, help="Prompt5")
    # parser.add_argument("--prompt6", type=str, default=None, help="Prompt6")

    args = parser.parse_args()

    # knowledge å°±æ˜¯å¯¹åº”ï¼šâ€œçŸ¥è¯†ç‚¹å›ç­”â€
    # test å¯¹åº”çš„å°±æ˜¯ï¼šâ€œæ™ºèƒ½å‡ºé¢˜â€

    if args.mode == "knowledge":
        try:
            with open(Path("data") / "Summary_of_Knowledge_Points.json", "r", encoding="utf-8") as f:
                knowledge_data = json.load(f)
        except FileNotFoundError:
            print("Error: JSON file not found.")
            exit(1)
    elif args.mode == "test":
        try:
            with open(Path("data") / "Simulated_Test.json", "r", encoding="utf-8") as f:
                knowledge_data = json.load(f)
        except FileNotFoundError:
            print("Error: JSON file not found.")
            exit(1)
    else:
        print("Error: Invalid mode. '-m' '--mode' Should input `test` or `knowledge`.")
        exit(1)

    # Embedding Model
    if args.Emodel == "default":
        document_store = ChromaDocumentStore()
        print("Now, Embedding Model is: all-MiniLM-L6-v2")
    else:
        # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥å‡½æ•°åˆ›å»º ChromaDocumentStore
        document_store = ChromaDocumentStore(
            embedding_function="OllamaEmbeddingFunction",
            url="http://localhost:11434/api/embeddings",
            model_name=args.Emodel
        )
        print(f"### Embedding Model is: {args.Emodel} ###")

    print(f"### Generator Model is: {args.Gmodel} ###")

    retriever = ChromaQueryTextRetriever(document_store=document_store)

    documents = []
    for item in knowledge_data:
        doc = Document(
            content=item["title"],
            meta={
                "é—®é¢˜/å†…å®¹": item["title"],
                "ç­”æ¡ˆ/å†…å®¹": item["content"],
                "éƒ¨åˆ†": item["part"]["section"],
                "é¢˜å‹/ç±»å‹": item["part"]["classification"],
                "å…³é”®è¯": ", ".join(item["tags"]) if isinstance(item["tags"], list) else item["tags"]
            }
        )
        documents.append(doc)

    # å°†æ–‡æ¡£å†™å…¥ ChromaDocumentStore
    for doc in tqdm(documents, desc="Indexing documents", unit="document"):
        document_store.write_documents([doc])

    # åˆ›å»º ChatPromptBuilder å’Œ OllamaChatGenerator
    prompt_builder = ChatPromptBuilder()
    generator = OllamaChatGenerator(
        model="qwen2:latest",
        # url="http://localhost:11434/api/chat",
        url="http://localhost:11434/",
        generation_kwargs={
            "temperature": 0.9,
        }
    )

    # åˆ›å»º Pipelineçš„éƒ¨åˆ†ä¿æŒä¸å˜
    pipe = Pipeline()
    pipe.add_component("prompt_builder", prompt_builder)
    pipe.add_component("llm", generator)
    pipe.connect("prompt_builder.prompt", "llm.messages")

    # è®¾ç½®å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡
    conversation_history = [
        ChatMessage.from_system("ä½ æ˜¯ä¸€ä¸ªç²¾é€šCè¯­è¨€çš„æ™ºèƒ½åŠ©æ•™ã€‚ç”¨æˆ·ä¼šæé—®ä¸€äº›å…³äºCè¯­è¨€ç¼–ç¨‹å’ŒæŠ€æœ¯æ–¹é¢çš„é—®é¢˜ï¼Œä½ è¦æä¾›æœ‰å¸®åŠ©çš„ç­”æ¡ˆã€‚åŒæ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªRAGçš„åœºæ™¯ï¼Œä¼šç»™å‡ºå¯¹åº”çš„RAGçš„å†…å®¹ã€‚è€Œè¿™ä¸ªåœºæ™¯ï¼Œæ­£æ˜¯â€œæ™ºèƒ½å‡ºé¢˜â€ï¼Œç”¨æˆ·ä¸€èˆ¬ä¼šé—®ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼Œç„¶åéœ€è¦ä½ ç»™å‡ºä¸€äº›é¢˜ç›®ï¼è€Œè¿™é‡ŒRAGå°±æ˜¯æœ‰ä¸€ä¸ªæ ‡å‡†é¢˜åº“ï¼Œä¼šæ£€ç´¢å‡ºå¯èƒ½åŒ¹é…çš„é—®é¢˜ï¼ˆä¹Ÿæœ‰å¯èƒ½å‡ºé”™ï¼‰ï¼Œè¯·ä½ å¯¹åˆé€‚çš„ä¾‹å­ç»™å‡ºï¼")
    ]

    def chat_with_RAG_first_time(question, conversation_history, retriever, top_k):
        # é¦–å…ˆï¼Œä½¿ç”¨ retriever è·å–ç›¸å…³æ–‡æ¡£
        retrieved_documents = retriever.run(query=question, top_k=top_k)["documents"]

        template = (
"## **é—®é¢˜**:\n" + question + "\n" +
"""
   ### æ³¨æ„äº‹é¡¹ï¼šï¼ˆå¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºå…·ä½“é—®é¢˜ï¼Œè€Œæ˜¯è¿›è¡Œé—®å€™æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œæ™ºèƒ½å‡ºé¢˜â€åœºæ™¯ï¼Œéœ€è¦ç»™å‡ºæ˜ç¡®é—®é¢˜æˆ–è€…çŸ¥è¯†ç‚¹ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦ç›´æ¥æŒ‰ç…§RAGå›ºå®šè¾“å‡ºçš„é¢˜ç›®ç»§ç»­è™šç©ºç­”é¢˜ï¼ï¼‰
   1. æ ¹æ®ç”¨æˆ·æŒ‡å®šé¢˜ç›®ç±»å‹ï¼ˆå¦‚é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€åˆ¤æ–­é¢˜ã€æ”¹é”™é¢˜ã€ä»£ç é¢˜ç­‰ï¼‰ã€éš¾åº¦å’Œæ•°é‡å‡ºé¢˜ã€‚è‹¥æœªæŒ‡å®šï¼Œéšæœºé€‰æ‹©é¢˜å‹ï¼Œé»˜è®¤ç”Ÿæˆä¸€é“ä¸­ç­‰éš¾åº¦çš„é¢˜ç›®ã€‚ 
   2. æ–‡æ¡£åŒ…å«å¤šä¸ªé¢˜ç›®æ¡ç›®ï¼Œè¯·æ ¹æ®å…³é”®è¯æ ‡ç­¾æ£€ç´¢å‡ºä¸é—®é¢˜æœ€ç›¸å…³çš„5æ¡å†…å®¹ä½œä¸ºå‚è€ƒï¼Œä½†è¿™å¹¶ä¸ä¸€å®šæ˜¯æœ€ç¬¦åˆã€è´´åˆ‡é—®é¢˜çš„ç­”æ¡ˆï¼Œè€Œæ˜¯ä¸€äº›ç¡®åˆ‡çš„ã€å‡†ç¡®çš„ç›¸å…³é¢˜ç›®ã€‚ 
   3. ç¡®ä¿æ‰€å‚è€ƒæ¡ç›®ä¸**ç”¨æˆ·é—®é¢˜**ä¿æŒä¸€è‡´ï¼Œé¿å…ä»…é å…³é”®è¯åŒ¹é…è€Œå¿½ç•¥ä¸Šä¸‹æ–‡æ„ä¹‰ã€‚
   4. æ ¸æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦åŒ…å«ä¸é—®é¢˜è¦æ±‚æ— å…³çš„é¢˜ç›®ï¼Œé¿å…è¯¯å¯¼ç­”æ¡ˆçš„ç”Ÿæˆã€‚  

   ### è¾“å‡ºæ ¼å¼ï¼š  
   - **é¢˜å¹²**ï¼šæ¸…æ™°æè¿°é¢˜ç›®è¦æ±‚å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œç¡®ä¿æ— æ­§ä¹‰æˆ–é”™è¯¯ï¼Œä¸”æœ‰å…¸å‹é”™è¯¯çš„å¹²æ‰°é¡¹ã€‚  
   - **å‚è€ƒç­”æ¡ˆ**ï¼šæä¾›æ˜ç¡®çš„å‚è€ƒç­”æ¡ˆï¼Œå¦‚æœ‰å¤šç§æ­£ç¡®ç­”æ¡ˆï¼Œå°½å¯èƒ½åœ°åŒ…å«ã€‚
   - **è§£é‡Š**ï¼š
      - è¯´æ˜è§£é¢˜æ€è·¯å’Œå…³é”®çŸ¥è¯†ç‚¹ã€‚  
      - æŒ‡å‡ºå¹²æ‰°é¡¹çš„å¸¸è§é”™è¯¯åŠå…¶åŸå› ã€‚
**æ³¨æ„**ï¼š
å¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºå…·ä½“é—®é¢˜ï¼Œè€Œæ˜¯è¿›è¡Œé—®å€™æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œæ™ºèƒ½å‡ºé¢˜â€åœºæ™¯ï¼Œéœ€è¦ç»™å‡ºæ˜ç¡®é—®é¢˜æˆ–è€…çŸ¥è¯†ç‚¹ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦ç›´æ¥æŒ‰ç…§RAGå›ºå®šè¾“å‡ºçš„é¢˜ç›®ç»§ç»­è™šç©ºç­”é¢˜ï¼
""")
        conversation_history.append(ChatMessage.from_user(template))

        # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ·»åŠ åˆ°å†å²å¯¹è¯ä¸­
        conversation_history.append(
            ChatMessage.from_system("RAGæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å¦‚ä¸‹ï¼šï¼ˆä¸ä¸€å®šæ˜¯ç™¾åˆ†ç™¾åŒ¹é…é—®é¢˜çš„ï¼è¯·ä½ è¿›è¡Œç”„åˆ«ï¼‰"))
        for doc in retrieved_documents:
            conversation_history.append(ChatMessage.from_system(f"{doc.meta['é—®é¢˜/å†…å®¹']}: {doc.content}"))

        # è·å–ç”Ÿæˆå™¨çš„å›ç­”
        querying = pipe.run(data={
            "prompt_builder": {
                "template_variables": {"question": question},
                "template": conversation_history
            }
        })

        # è·å–å›ç­”å¹¶æ›´æ–°å†å²å¯¹è¯
        response = querying["llm"]["replies"]
        response_text = response[0].text
        conversation_history.append(ChatMessage.from_assistant(response_text))

        return response_text, conversation_history


    def chat_with_RAG_second_time(question, conversation_history, retriever, top_k):
        # é¦–å…ˆï¼Œä½¿ç”¨ retriever è·å–ç›¸å…³æ–‡æ¡£
        retrieved_documents = retriever.run(query=question, top_k=top_k)["documents"]

        template = (
"## **é—®é¢˜**:\n" + question + "\n" +
"""
   ### æ³¨æ„äº‹é¡¹ï¼šï¼ˆå¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºå…·ä½“é—®é¢˜ï¼Œè€Œæ˜¯è¿›è¡Œé—®å€™æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œæ™ºèƒ½å‡ºé¢˜â€åœºæ™¯ï¼Œéœ€è¦ç»™å‡ºæ˜ç¡®é—®é¢˜æˆ–è€…çŸ¥è¯†ç‚¹ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦ç›´æ¥æŒ‰ç…§RAGå›ºå®šè¾“å‡ºçš„é¢˜ç›®ç»§ç»­è™šç©ºç­”é¢˜ï¼ï¼‰
   1. æ ¹æ®ç”¨æˆ·æŒ‡å®šé¢˜ç›®ç±»å‹ï¼ˆå¦‚é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€åˆ¤æ–­é¢˜ã€æ”¹é”™é¢˜ã€ä»£ç é¢˜ç­‰ï¼‰ã€éš¾åº¦å’Œæ•°é‡å‡ºé¢˜ã€‚è‹¥æœªæŒ‡å®šï¼Œéšæœºé€‰æ‹©é¢˜å‹ï¼Œé»˜è®¤ç”Ÿæˆä¸€é“ä¸­ç­‰éš¾åº¦çš„é¢˜ç›®ã€‚ 
   2. æ–‡æ¡£åŒ…å«å¤šä¸ªé¢˜ç›®æ¡ç›®ï¼Œè¯·æ ¹æ®å…³é”®è¯æ ‡ç­¾æ£€ç´¢å‡ºä¸é—®é¢˜æœ€ç›¸å…³çš„5æ¡å†…å®¹ä½œä¸ºå‚è€ƒï¼Œä½†è¿™å¹¶ä¸ä¸€å®šæ˜¯æœ€ç¬¦åˆã€è´´åˆ‡é—®é¢˜çš„ç­”æ¡ˆï¼Œè€Œæ˜¯ä¸€äº›ç¡®åˆ‡çš„ã€å‡†ç¡®çš„ç›¸å…³é¢˜ç›®ã€‚ 
   3. ç¡®ä¿æ‰€å‚è€ƒæ¡ç›®ä¸**ç”¨æˆ·é—®é¢˜**ä¿æŒä¸€è‡´ï¼Œé¿å…ä»…é å…³é”®è¯åŒ¹é…è€Œå¿½ç•¥ä¸Šä¸‹æ–‡æ„ä¹‰ã€‚
   4. æ ¸æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦åŒ…å«ä¸é—®é¢˜è¦æ±‚æ— å…³çš„é¢˜ç›®ï¼Œé¿å…è¯¯å¯¼ç­”æ¡ˆçš„ç”Ÿæˆã€‚  

   ### è¾“å‡ºæ ¼å¼ï¼š  
   - **é¢˜å¹²**ï¼šæ¸…æ™°æè¿°é¢˜ç›®è¦æ±‚å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œç¡®ä¿æ— æ­§ä¹‰æˆ–é”™è¯¯ï¼Œä¸”æœ‰å…¸å‹é”™è¯¯çš„å¹²æ‰°é¡¹ã€‚  
   - **å‚è€ƒç­”æ¡ˆ**ï¼šæä¾›æ˜ç¡®çš„å‚è€ƒç­”æ¡ˆï¼Œå¦‚æœ‰å¤šç§æ­£ç¡®ç­”æ¡ˆï¼Œå°½å¯èƒ½åœ°åŒ…å«ã€‚
   - **è§£é‡Š**ï¼š
      - è¯´æ˜è§£é¢˜æ€è·¯å’Œå…³é”®çŸ¥è¯†ç‚¹ã€‚  
      - æŒ‡å‡ºå¹²æ‰°é¡¹çš„å¸¸è§é”™è¯¯åŠå…¶åŸå› ã€‚
**æ³¨æ„**ï¼š
å¦‚æœå¹¶æ²¡æœ‰ç»™å‡ºå…·ä½“é—®é¢˜ï¼Œè€Œæ˜¯è¿›è¡Œé—®å€™æˆ–è€…å…¶ä»–è¡Œä¸ºï¼Œè¯·é‡ç”³å½“å‰æ˜¯â€œæ™ºèƒ½å‡ºé¢˜â€åœºæ™¯ï¼Œéœ€è¦ç»™å‡ºæ˜ç¡®é—®é¢˜æˆ–è€…çŸ¥è¯†ç‚¹ï¼é™¤éè¦æ±‚ï¼Œè¯·ä¸è¦ç›´æ¥æŒ‰ç…§RAGå›ºå®šè¾“å‡ºçš„é¢˜ç›®ç»§ç»­è™šç©ºç­”é¢˜ï¼
""")
        conversation_history.append(ChatMessage.from_user(template))

        # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ·»åŠ åˆ°å†å²å¯¹è¯ä¸­
        conversation_history.append(
            ChatMessage.from_system("RAGæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å¦‚ä¸‹ï¼šï¼ˆä¸ä¸€å®šæ˜¯ç™¾åˆ†ç™¾åŒ¹é…é—®é¢˜çš„ï¼è¯·ä½ è¿›è¡Œç”„åˆ«ï¼‰"))
        for doc in retrieved_documents:
            conversation_history.append(ChatMessage.from_system(f"{doc.meta['é—®é¢˜/å†…å®¹']}: {doc.content}"))

        # è·å–ç”Ÿæˆå™¨çš„å›ç­”
        querying = pipe.run(data={
            "prompt_builder": {
                "template_variables": {"question": question},
                "template": conversation_history
            }
        })

        # è·å–å›ç­”å¹¶æ›´æ–°å†å²å¯¹è¯
        response = querying["llm"]["replies"]
        response_text = response[0].text
        conversation_history.append(ChatMessage.from_assistant(response_text))

        return response_text, conversation_history



    # äº¤äº’å¼å¯¹è¯å¾ªç¯
    print("\nğŸŸ¢ è¶…çº§ C è¯­è¨€åŠ©æ•™å·²å°±ç»ªï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰")
    i = 1

    while True:
        try:
            print(f"\n## Round {i} ##")
            question = input("\nâ“ ä½ çš„é—®é¢˜: \n")
            if question.lower() in ["exit", "quit"]:
                print("é€€å‡ºå¯¹è¯ã€‚")
                break

            # ä¿å­˜å½“å‰çš„ç³»ç»Ÿä¿¡æ¯åˆ°èŠå¤©è®°å½•
            chat_record = {
                "timestamp": time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime()),
                "user_input": question,
                "system_response": ""
            }

            if i == 1:
                # å¼€å§‹ç¬¬ä¸€è½®å¯¹è¯
                answer, conversation_history = chat_with_RAG_first_time(
                    question=question,
                    conversation_history=conversation_history,
                    retriever=retriever,
                    top_k=args.top_k
                )

            else:
                # éç¬¬ä¸€è½®å¯¹è¯
                answer, conversation_history = chat_with_RAG_second_time(
                    question=question,
                    conversation_history=conversation_history,
                    retriever=retriever,
                    top_k=args.top_k
                )

            # æ›´æ–°èŠå¤©è®°å½•ä¸­çš„ç³»ç»Ÿå“åº”
            chat_record["system_response"] = answer

            # å°†å½“å‰èŠå¤©è®°å½•æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            chat_records.append(chat_record)

            # å°†èŠå¤©è®°å½•ä¿å­˜åˆ° JSON æ–‡ä»¶
            with open(os.path.join(output_folder, "chat_history.json"), "w", encoding="utf-8") as f:
                json.dump(chat_records, f, ensure_ascii=False, indent=2)

            # è¾“å‡ºå›ç­”
            print(f"\nğŸ¤– åŠ©æ‰‹å›ç­”:\n{answer}")

            i += 1

        except KeyboardInterrupt:
            print("\nå¯¹è¯ä¸­æ–­ã€‚")
            break


if __name__ == "__main__":
    main()
